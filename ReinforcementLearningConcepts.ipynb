{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReinforcementLearningConcepts.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabeMaldonado/JupyterNotebooks/blob/master/ReinforcementLearningConcepts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeO6j4wOPwlL",
        "colab_type": "text"
      },
      "source": [
        "# REINFORCEMENT LEARNING\n",
        "\n",
        "## AWS DeepRacer Notes\n",
        "\n",
        "### What is reinforcement learning?\n",
        "\n",
        "Reinforcement Learning (RL) is a type of Machine Learning in which an *agent* explores an *environment*  to learn how to perform desired tasks by taking actions with good outcomes and avoiding action with bad outcomes. \n",
        "\n",
        "An RL model will learn from its experience and over time-- will be able to identify which actions lead to the best results.\n",
        "\n",
        "Other types of Machine Learning:\n",
        "\n",
        "\n",
        "*   Supervised Learning -- example-driven learning, with labeled data of known outputs for given inputs, where a model is trained to predict outcomes from new inputs.\n",
        "*   Unsupervised Learning -- inference-based training, with unlabeled data without known outputs, where a model is trained to identify realted structures or similar patterns within the input data.\n",
        "\n",
        "### How does AWS DeepRacer learn how to drive by itself?\n",
        "\n",
        "In RL the agent interacts with an environment with the objective to maximize its total reward. \n",
        "The agent takes an *action* * based on the environment *state* and the environment returns the reward and the next state. The agent learns from trial and error, initially taking random actins and over time identifying the actions that lead to long-term rewards. \n",
        "\n",
        "### AGENT\n",
        "The *agent* simulates the AWS DeepRacer vehicle in the simulation for the training. More especifically, it embodies the neural network that controls the vehicle, taking inputs and deciding actions.\n",
        "\n",
        "### ENVIRONMENT\n",
        "The *environment* contains a track that defines where the agent can go and what state it can be in. The agent explores the environment to collect data to train the underlying neural network.\n",
        "\n",
        "### STATE \n",
        "A *state* represents a snapshot of the environment the agent is in at a point in time. For AWS DeepRacer, a state is an image captured by the front-facing camera of the vehicle. \n",
        "\n",
        "### ACTION\n",
        "An *action* is a move made by the agent in the current state. For the AWS DeepRacer, an action represents a move at a particular speed and steering angle.\n",
        "\n",
        "### REWARD\n",
        "A *reward* is the score given as feedback to the agent when it takes an action in a given state. In training the AWS DeepRacer model, the reward is returned by a *reward function*. In general, you decide or supply a reward function to specify what is desirable or undesirable action for the agent to take in a given state. \n",
        "\n",
        "## TRAINING AN RL MODEL\n",
        "Training is an iterative process. In a simulator, the agent explores the environment and builds up experience. The experiences collected are used to update the neaural network periodically and the updated models are used to create more experiences. \n",
        "With AWS DeepRacer, we are training a vehicle to drive itslef. Let's explore this concept with a simplified example.\n",
        "\n",
        "### A Simplified Environment\n",
        "In this example, we want the vehicle to travel from point A to point B following the shortest path. Let's pretend we have a grid of squares and that each square represents an individual state and we will allow the vehicle to move up or down while facing the direction of the goal.\n",
        "\n",
        "### Scores\n",
        "We can assign a score to each square in the grid to decide wich behavior to incentivize. We can designate the squares on the edge of the track as 'stop states' which will tell the vehicle that it has gone off the track and failed. \n",
        "Since we want to incentivize the vehicle to learn to drive down the center of the track, we assign a high reward for the squares on the center line and a low reward elsewhere.\n",
        "\n",
        "### An Episode\n",
        "In RL training, the vehicle will start by exploring the grid until it moves out of bounds or reaches its destination.\n",
        "As it drives around, the vehicle accumulates rewards from the scores we defined. This process is called an *episode*.\n",
        "\n",
        "### Iteration\n",
        "RL algorithms are trained by repeated optimization of cumulative rewards. \n",
        "The model will learn which action (and subsequent actions) will result in the highest cummulative reward on the way to the goal.\n",
        "Learning does not just happen on the first go, it takes some iteration. First, the agent needs to explore the environment and see where it can get the highests rewards, before it can exploit that knowledge. \n",
        "\n",
        "### Exploration\n",
        "As the agent gains more and more experience, it learns to stay on the central squares to get higher rewards.\n",
        "If we plot the results from each episode , we can see how the model performs and improves over time.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA0AZOnoPnLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}